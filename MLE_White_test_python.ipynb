{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab37f7e-2ad8-4c05-ba6d-c57a02d3a898",
   "metadata": {},
   "source": [
    "# Code Explanation\n",
    "\n",
    "This script implements the **White Test for Heteroscedasticity**, a statistical procedure to assess whether the variance of residuals in a regression model is constant (homoscedasticity). The test checks for relationships between residual variance and the independent variables, their squares, and their cross-products.\n",
    "\n",
    "## `white_test` Function\n",
    "\n",
    "### 1. **Inputs**\n",
    "- **`data`**: Dataset where the first column is the dependent variable (\\(Y\\)), and the remaining columns are independent variables.\n",
    "- **`model_results`**: Regression output (e.g., coefficients and residuals) from a function like `linear_model` or `mle_model`.\n",
    "- **`feature_names`** *(optional)*: List of variable names, including the dependent variable.\n",
    "\n",
    "### 2. **Steps to Perform the White Test**\n",
    "\n",
    "#### **Data Preparation**\n",
    "- Converts the input to a numpy array if provided as a pandas DataFrame.\n",
    "- Constructs the design matrix (\\(X\\)) with an intercept term.\n",
    "\n",
    "#### **Residual Calculation**\n",
    "- Uses the regression coefficients from `model_results` to compute predicted values.\n",
    "- Calculates residuals as \\(Y - X\\beta\\) and squares them (\\(e^2\\)) for the test.\n",
    "\n",
    "#### **White Test Regressors**\n",
    "- Constructs auxiliary regressors:\n",
    "  - Squares of independent variables.\n",
    "  - Cross-products of independent variables (combinations with replacement).\n",
    "- Combines these with the original regressors to form a new design matrix (\\(X_{\\text{White}}\\)).\n",
    "\n",
    "#### **Auxiliary Regression**\n",
    "- Fits a regression model where the dependent variable is the squared residuals (\\(e^2\\)), and the independent variables are the regressors from \\(X_{\\text{White}}\\).\n",
    "- Calculates the test statistic:\n",
    "  \\[\n",
    "  W = n \\cdot R^2\n",
    "  \\]\n",
    "  - \\(n\\): Number of observations.\n",
    "  - \\(R^2\\): Coefficient of determination from the auxiliary regression.\n",
    "\n",
    "#### **Statistical Inference**\n",
    "- Degrees of freedom (\\(df\\)) are equal to the number of regressors in the auxiliary model.\n",
    "- Calculates the \\(p\\)-value using the chi-squared distribution:\n",
    "  \\[\n",
    "  p = 1 - \\text{CDF}_{\\chi^2}(W, df)\n",
    "  \\]\n",
    "- Determines whether to reject the null hypothesis of homoscedasticity at a significance level (\\(\\alpha = 0.05\\)).\n",
    "\n",
    "### 3. **Output**\n",
    "- Returns a dictionary containing:\n",
    "  - **Summary**: Text-based explanation of the test results.\n",
    "  - **White Test Statistic**: Value of \\(W\\).\n",
    "  - **Degrees of Freedom**: Number of regressors in the auxiliary regression.\n",
    "  - **P-value**: Probability of observing the test statistic under the null hypothesis.\n",
    "  - **Conclusion**: Whether homoscedasticity is rejected.\n",
    "  - **Auxiliary \\(R^2\\)**: Goodness-of-fit from the auxiliary regression.\n",
    "  - **Auxiliary Model**: Statsmodels object of the auxiliary regression.\n",
    "\n",
    "### 4. **Error Handling**\n",
    "- Catches linear algebra errors (e.g., singular matrix) that may occur due to perfect multicollinearity in the auxiliary regression.\n",
    "- Provides a descriptive message when the test cannot be performed.\n",
    "\n",
    "### 5. **Use Case**\n",
    "The White Test is a diagnostic tool to check for heteroscedasticity in regression models. It is especially useful in cases where residual variance might depend on the independent variables or their combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ce2af-408f-41b8-8773-70f16eb98981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def mle_model(data, feature_names=None):\n",
    "    \"\"\"\n",
    "    Maximum Likelihood Estimation (MLE) for Linear Model\n",
    "    \"\"\"\n",
    "    # Data preparation\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if feature_names is None:\n",
    "            feature_names = data.columns.tolist()\n",
    "        data = data.values\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = ['Y'] + [f'X{i}' for i in range(1, data.shape[1])]\n",
    "    \n",
    "    if data.shape[1] < 2:\n",
    "        raise ValueError(\"The data must contain at least two columns\")\n",
    "    \n",
    "    # Extract variables\n",
    "    y = data[:, 0].reshape(-1, 1)\n",
    "    X = np.column_stack([np.ones(len(data)), data[:, 1:]])\n",
    "    \n",
    "    def log_likelihood(params):\n",
    "        \"\"\"Negative log-likelihood function\"\"\"\n",
    "        beta = params[:-1].reshape(-1, 1)\n",
    "        sigma = np.exp(params[-1])  # Ensure sigma is positive\n",
    "        residuals = y - X @ beta\n",
    "        ll = -0.5 * len(X) * np.log(2 * np.pi * sigma**2) - \\\n",
    "             (1 / (2 * sigma**2)) * np.sum(residuals**2)\n",
    "        return -ll  # Return negative because we're minimizing\n",
    "    \n",
    "    # Initialize parameters (beta and log(sigma))\n",
    "    init_params = np.zeros(X.shape[1] + 1)\n",
    "    init_params[-1] = np.log(np.std(y))\n",
    "    \n",
    "    # Optimize using MLE\n",
    "    mle_result = optimize.minimize(\n",
    "        log_likelihood,\n",
    "        init_params,\n",
    "        method='BFGS',\n",
    "        options={'disp': False}\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    beta_mle = mle_result.x[:-1].reshape(-1, 1)\n",
    "    sigma_mle = np.exp(mle_result.x[-1])\n",
    "    log_lik = -mle_result.fun\n",
    "    \n",
    "    # Calculate standard errors using OLS approach\n",
    "    y_hat = X @ beta_mle\n",
    "    residuals = y - y_hat\n",
    "    n = len(X)\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.sum(residuals**2) / (n - p)\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    try:\n",
    "        XtX_inv = np.linalg.inv(X.T @ X)\n",
    "        se_beta_mle = np.sqrt(np.diag(XtX_inv * mse))\n",
    "        \n",
    "        # Calculate test statistics\n",
    "        t_values = beta_mle.flatten() / se_beta_mle\n",
    "        p_values = 2 * stats.t.sf(np.abs(t_values), df=n-p)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: Could not compute standard errors (singular matrix)\")\n",
    "        se_beta_mle = np.full(len(beta_mle), np.nan)\n",
    "        t_values = np.full(len(beta_mle), np.nan)\n",
    "        p_values = np.full(len(beta_mle), np.nan)\n",
    "    \n",
    "    # Calculate R-squared statistics\n",
    "    SS_tot = np.sum((y - np.mean(y))**2)\n",
    "    SS_res = np.sum(residuals**2)\n",
    "    r_squared = 1 - (SS_res / SS_tot)\n",
    "    adj_r_squared = 1 - (1 - r_squared) * ((n - 1) / (n - p))\n",
    "    \n",
    "    # Calculate AIC and BIC\n",
    "    k = len(init_params)  # number of parameters (including sigma)\n",
    "    aic = 2 * k - 2 * log_lik\n",
    "    bic = np.log(n) * k - 2 * log_lik\n",
    "    \n",
    "    # Create diagnostic plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Residuals vs Fitted\n",
    "    axes[0,0].scatter(y_hat.flatten(), residuals.flatten())\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,0].set_xlabel('Fitted values')\n",
    "    axes[0,0].set_ylabel('Residuals')\n",
    "    axes[0,0].set_title('Residuals vs Fitted')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals.flatten(), dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title('Normal Q-Q Plot')\n",
    "    \n",
    "    # Scale-Location plot\n",
    "    axes[1,0].scatter(y_hat.flatten(), np.sqrt(np.abs(residuals.flatten())))\n",
    "    axes[1,0].set_xlabel('Fitted values')\n",
    "    axes[1,0].set_ylabel('âˆš|Residuals|')\n",
    "    axes[1,0].set_title('Scale-Location Plot')\n",
    "    \n",
    "    # Density plot of residuals\n",
    "    sns.kdeplot(data=residuals.flatten(), ax=axes[1,1])\n",
    "    axes[1,1].set_title('Residuals Density Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create coefficient table\n",
    "    coef_data = []\n",
    "    var_names = ['Intercept'] + feature_names[1:]\n",
    "    for i in range(len(beta_mle)):\n",
    "        coef_data.append([\n",
    "            var_names[i],\n",
    "            f\"{beta_mle[i][0]:.4f}\",\n",
    "            f\"{se_beta_mle[i]:.4f}\",\n",
    "            f\"{t_values[i]:.4f}\",\n",
    "            f\"{p_values[i]:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    return {\n",
    "        'summary': f\"\"\"\n",
    "Maximum Likelihood Estimation Results\n",
    "===================================\n",
    "Dependent Variable: {feature_names[0]}\n",
    "Number of Observations: {len(X)}\n",
    "Number of Predictors: {X.shape[1]-1}\n",
    "\n",
    "Model Statistics:\n",
    "----------------\n",
    "R-squared: {r_squared:.4f}\n",
    "Adjusted R-squared: {adj_r_squared:.4f}\n",
    "Log-likelihood: {log_lik:.4f}\n",
    "AIC: {aic:.4f}\n",
    "BIC: {bic:.4f}\n",
    "Sigma (MLE): {sigma_mle:.4f}\n",
    "\n",
    "Coefficients:\n",
    "------------\n",
    "{tabulate(coef_data, headers=['Variable', 'Coefficient', 'Std. Error', 't-value', 'p-value'], \n",
    "          tablefmt='pipe', floatfmt='.4f')}\n",
    "\"\"\",\n",
    "        'coefficients': pd.DataFrame(coef_data, \n",
    "                                   columns=['Variable', 'Coefficient', 'Std. Error', 't-value', 'p-value']),\n",
    "        'r_squared': r_squared,\n",
    "        'adj_r_squared': adj_r_squared,\n",
    "        'log_likelihood': log_lik,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'sigma': sigma_mle,\n",
    "        'residuals': residuals.flatten(),\n",
    "        'fitted_values': y_hat.flatten(),\n",
    "        'diagnostic_plots': fig\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a36e9-323f-4b38-b123-20441ce1a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations_with_replacement\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def white_test(data, model_results, feature_names=None):\n",
    "    \"\"\"\n",
    "    White Test for Heteroscedasticity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy.ndarray or pandas.DataFrame\n",
    "        Data where the first column is the dependent variable (Y) and \n",
    "        the remaining columns are the independent variables\n",
    "    model_results : dict\n",
    "        Results from the mle_model or linear_model function\n",
    "    feature_names : list, optional\n",
    "        Names of the features (including dependent variable)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the White test statistic, p-value, and conclusion\n",
    "    \"\"\"\n",
    "    # Data preparation\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        if feature_names is None:\n",
    "            feature_names = data.columns.tolist()\n",
    "        data = data.values\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = ['Y'] + [f'X{i}' for i in range(1, data.shape[1])]\n",
    "    \n",
    "    # Extract variables\n",
    "    y = data[:, 0].reshape(-1, 1)\n",
    "    X = np.column_stack([np.ones(len(data)), data[:, 1:]])\n",
    "    \n",
    "    # Extract estimated coefficients and calculate residuals\n",
    "    beta = model_results['coefficients']['Coefficient'].values\n",
    "    beta = np.array([float(b) for b in beta]).reshape(-1, 1)\n",
    "    residuals = y - X @ beta\n",
    "    \n",
    "    # Calculate squared residuals\n",
    "    residuals_squared = residuals**2\n",
    "    \n",
    "    # Construct White test regressors\n",
    "    X_no_constant = X[:, 1:]  # Remove constant term\n",
    "    n, k = X_no_constant.shape\n",
    "    \n",
    "    # Create squares and cross-products\n",
    "    squares = X_no_constant**2\n",
    "    \n",
    "    # Create cross-products\n",
    "    cross_products = []\n",
    "    for i, j in combinations_with_replacement(range(k), 2):\n",
    "        if i != j:\n",
    "            cross_products.append(X_no_constant[:, i] * X_no_constant[:, j])\n",
    "    \n",
    "    if cross_products:\n",
    "        cross_products = np.column_stack(cross_products)\n",
    "        X_white = np.column_stack([X, squares, cross_products])\n",
    "    else:\n",
    "        X_white = np.column_stack([X, squares])\n",
    "    \n",
    "    # Fit auxiliary regression\n",
    "    try:\n",
    "        auxiliary_model = sm.OLS(residuals_squared, X_white).fit()\n",
    "        \n",
    "        # Calculate White test statistic\n",
    "        white_statistic = auxiliary_model.nobs * auxiliary_model.rsquared\n",
    "        df = auxiliary_model.df_model  # Degrees of freedom\n",
    "        white_pvalue = stats.chi2.sf(white_statistic, df)\n",
    "        \n",
    "        # Create conclusion\n",
    "        alpha = 0.05  # significance level\n",
    "        conclusion = (\"Reject null hypothesis of homoscedasticity\" \n",
    "                     if white_pvalue < alpha \n",
    "                     else \"Fail to reject null hypothesis of homoscedasticity\")\n",
    "        \n",
    "        # Create detailed results\n",
    "        results = {\n",
    "            'summary': f\"\"\"\n",
    "White Test for Heteroscedasticity\n",
    "================================\n",
    "H0: Homoscedasticity (constant variance)\n",
    "H1: Heteroscedasticity (non-constant variance)\n",
    "\n",
    "Test Results:\n",
    "------------\n",
    "White test statistic: {white_statistic:.4f}\n",
    "Degrees of freedom: {df}\n",
    "P-value: {white_pvalue:.4f}\n",
    "\n",
    "Conclusion (Î± = {alpha}):\n",
    "{conclusion}\n",
    "\"\"\",\n",
    "            'white_statistic': white_statistic,\n",
    "            'degrees_of_freedom': df,\n",
    "            'p_value': white_pvalue,\n",
    "            'conclusion': conclusion,\n",
    "            'auxiliary_r_squared': auxiliary_model.rsquared,\n",
    "            'auxiliary_model': auxiliary_model\n",
    "        }\n",
    "        \n",
    "    except np.linalg.LinAlgError:\n",
    "        results = {\n",
    "            'summary': \"\"\"\n",
    "White Test for Heteroscedasticity\n",
    "================================\n",
    "Error: Singular matrix encountered. Cannot perform White test.\n",
    "This might be due to perfect multicollinearity in the auxiliary regression.\n",
    "\"\"\",\n",
    "            'white_statistic': None,\n",
    "            'degrees_of_freedom': None,\n",
    "            'p_value': None,\n",
    "            'conclusion': \"Test could not be performed due to computational issues\",\n",
    "            'auxiliary_r_squared': None,\n",
    "            'auxiliary_model': None\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
